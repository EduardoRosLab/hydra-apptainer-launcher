# Hydra Apptainer Launcher ðŸðŸ“¦ðŸš€

[Hydra](https://hydra.cc) is a powerful framework from Meta for elegantly configuring complex applications. It lets you compose configurations dynamically, override parameters from the command line, and run hyperparameter sweeps with minimal code changes.

This plugin extends Hydra's job launching capabilities by transparently executing your Python scripts inside Apptainer containers on HPC systems â€” ensuring all dependencies are available on compute nodes that lack your development environment.

**Features:**
- ðŸš€ Launch SLURM jobs that run inside Apptainer containers
- ðŸ”§ Full control over SLURM resources (GPUs, CPUs, memory, partitions)
- ðŸ”„ Native support for Hydra multirun (hyperparameter sweeps)
- ðŸ“¦ Zero code changes to your Hydra applications
- ðŸ§ª Local testing mode without SLURM


## The Problem

On HPC clusters, **compute nodes don't have your Python dependencies installed**. Apptainer containers carry the entire runtime environment to every node, and this plugin makes Hydra launch everything inside those containers transparently.

```
Login Node: atchbp                        Compute Node: compute-0-[0-10]
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ python train.py  â”‚    SLURM job       â”‚ apptainer exec --nv container.sif   â”‚
â”‚                  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º    â”‚   python train.py                   â”‚
â”‚ (launches job,   â”‚   (sbatch)         â”‚                                     â”‚
â”‚  does NOT run    â”‚                    â”‚ (runs INSIDE the container where    â”‚
â”‚  the training)   â”‚                    â”‚  all dependencies are installed)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                         â”‚
   Shared Filesystem: /path/to/my_project.sif  â—„â”€â”€â”€â”˜
```

## Install

### Prerequisites

On the **login node** (where you launch jobs):
- A python virtual environment so you can install the plugin
- Access to `sbatch` (SLURM commands)
- The `.sif` container file on a shared filesystem

**You do NOT need your project's heavy dependencies (JAX, PyTorch, etc.) on the login node.** Those live inside the container. You only need Hydra and this plugin to *submit* jobs.

### Installation on the venv
```bash
pip install git+https://github.com/EduardoRosLab/hydra-apptainer-launcher.git
```

Or for development or testing:
```bash
git clone https://github.com/EduardoRosLab/hydra-apptainer-launcher.git
pip install -e ./hydra-apptainer-launcher
```

## Quick Start

### 1. Create your Hydra app

```python
# scripts/train.py
import hydra
from omegaconf import DictConfig

@hydra.main(version_base=None, config_path=".", config_name="config") # Hydra entry point. This is where the magic happens.
def train(cfg: DictConfig) -> None:
    import torch  # imported INSIDE the container on the compute node
    print(f"Training with lr={cfg.lr} on {torch.cuda.device_count()} GPUs")

if __name__ == "__main__":
    train()
```

### 2. Create your app config

This YAML file contains your application's hyperparameters.

```yaml
# scripts/config.yaml
lr: 0.001
batch_size: 256
```

### 3. Create a launcher config

This YAML file tells Hydra to use the Apptainer + SLURM launcher and specify the SLURM resources. Check [submitit-slurm-launch](https://hydra.cc/docs/plugins/submitit_launcher/) for more details on available parameters.

```yaml
# scripts/hydra/launcher/submitit_apptainer.yaml
_target_: hydra_plugins.hydra_apptainer_launcher.submitit_launcher.CustomSlurmLauncher
submitit_folder: ${hydra.sweep.dir}/.submitit/%j
timeout_min: 360
partition: full
python: "apptainer exec --nv ${hydra.runtime.cwd}/my_project.sif python"
```

### Project Structure

After installing the plugin, your project needs:

1. Launcher YAML file(s) in `scripts/hydra/launcher/` â€” adapted to your resources
2. A `Dockerfile` that installs your dependencies + this plugin (see [templates](examples/templates/))
3. A `container.sh` to build the `.sif` image

```
my_project/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train.py
â”‚   â”œâ”€â”€ config.yaml
â”‚   â””â”€â”€ hydra/
â”‚       â””â”€â”€ launcher/
â”‚           â””â”€â”€ submitit_apptainer.yaml
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ container.sh
â””â”€â”€ requirements.txt    # includes: hydra-apptainer-launcher
```


### 4. Run

Hydra allows you to do parameter sweeps easily. Each combination becomes a separate SLURM job, each running inside the container.

```bash
# Single job on SLURM inside the container
python scripts/train.py -m hydra/launcher=submitit_apptainer

# Parameter sweep (each combination = 1 SLURM job)
python scripts/train.py -m hydra/launcher=submitit_apptainer lr=0.001,0.0001 batch_size=128,512

#parameter sweep with each job in a different node
python3 scripts/train.py -m \
    hydra/launcher=submitit_apptainer \
    +'hydra.launcher.additional_parameters={exclusive: ""}' \
    lr=0.001,0.0001 batch_size=128,512
```



### Output Structure

After running a multirun sweep, Hydra creates the following directory structure for each job:

```
multirun/
â””â”€â”€ 2024-01-15/
    â””â”€â”€ 12-34-56/                    # Timestamp of launch
        â”œâ”€â”€ .submitit/               # Submitit JOBS and launched script
        â”‚   â”œâ”€â”€ JOB_ID/*sh          # SLURM job scripts
        â”‚   â””â”€â”€ JOB_ID_0/*        # Output and error logs per job 0
                    
        â”œâ”€â”€ 0/                       # First job (lr=0.001, batch_size=128)
        â”‚   â”œâ”€â”€ .hydra/
        â”‚   â”‚   â”œâ”€â”€ config.yaml      # Resolved config for this job
        â”‚   â”‚   â”œâ”€â”€ hydra.yaml
        â”‚   â”‚   â””â”€â”€ overrides.yaml
        â”‚   â””â”€â”€ train.log            # Your application output
        â”œâ”€â”€ 1/                       # Second job (lr=0.001, batch_size=512)
        â”‚   â”œâ”€â”€ .hydra/
        â”‚   â””â”€â”€ train.log
        â”œâ”€â”€ 2/                       # Third job (lr=0.0001, batch_size=128)
        â”‚   â”œâ”€â”€ .hydra/
        â”‚   â””â”€â”€ train.log
        â”œâ”€â”€ 3/                       # Fourth job (lr=0.0001, batch_size=512)
        â”‚   â”œâ”€â”€ .hydra/
        â”‚   â””â”€â”€ train.log
        â”œâ”€â”€ .hydra/
        â”‚   â””â”€â”€ config.yaml          # Multirun config
        â”œâ”€â”€ multirun.yaml            # Summary of all runs
        â””â”€â”€ optimization_results.yaml # Optional: results aggregation
```

Each numbered subdirectory (`0/`, `1/`, `2/`, ...) corresponds to one parameter combination and contains:
- `.hydra/config.yaml` â€” the fully resolved configuration for that specific run
- Your application's outputs (logs, checkpoints, etc.)

---

## How It Works

The official `hydra-submitit-launcher` plugin assumes the compute node has the same Python environment as the login node. On HPC clusters, that's not the case.

This plugin adds a **`python` parameter** that replaces the Python command submitit uses:

```
Without this plugin (official):
  submitit generates â†’ python -u submitit_pickled_job.py

With this plugin:
  submitit generates â†’ apptainer exec --nv container.sif python -u submitit_pickled_job.py
```

The entire execution â€” deserialization, config loading, your function â€” happens inside the container where all dependencies are installed.

### Full Flow

```
You (login node):
  python train.py -m hydra/launcher=submitit_apptainer lr=0.001,0.0001
     â”‚
     â–¼
Hydra: sees -m (multirun) â†’ generates 2 configs (lr=0.001, lr=0.0001)
     â”‚    â†’ delegates to the launcher plugin specified in the YAML
     â–¼
hydra-apptainer-launcher: creates submitit AutoExecutor
     â”‚    â†’ builds job parameters for each config combination
     â”‚    â†’ sets python="apptainer exec --nv container.sif python"
     â–¼
submitit: generates sbatch script for each job
     â”‚    â†’ serializes (pickles) the task function + config
     â”‚    â†’ calls sbatch to submit to SLURM
     â–¼
SLURM: schedules jobs on compute nodes
     â”‚
     â–¼
Compute node: runs the sbatch script, which calls:
  apptainer exec --nv /path/container.sif python -u submitit_pickled_job.py
     â”‚
     â–¼
Inside the container:
  - submitit deserializes the task function and config
  - Hydra applies the sweep overrides (lr=0.001 for job 1, lr=0.0001 for job 2)
  - Your training function runs with full access to all installed dependencies
  - Results are written to the shared filesystem
     â”‚
     â–¼
Login node: submitit collects results from the shared filesystem
```

---


## Examples

See the [examples/](examples/) directory for runnable examples:

- **[hello_cluster](examples/hello_cluster/)** â€” Minimal app to verify the full pipeline works
- **[gpu_training](examples/gpu_training/)** â€” Simulated GPU training with hyperparameter sweeps
- **[templates](examples/templates/)** â€” Dockerfile and container build script templates

### Parameter Sweeps

Each parameter combination becomes a separate SLURM job, each running inside the container:

```bash
# 12 jobs = 3 lr x 2 batch_size x 2 seeds
python scripts/train.py -m \
  hydra/launcher=submitit_apptainer \
  lr=0.001,0.0003,0.0001 \
  batch_size=128,512 \
  seed=1,2
```


## Testing

```bash
pip install -r requirements-dev.txt

# Unit and local integration tests (no cluster needed)
pytest tests/test_plugin_discovery.py tests/test_local_launcher.py -v

# HPC cluster test (run ON the cluster, requires a .sif image)
pytest tests/test_slurm_cluster.py -v -m slurm --sif-path /path/to/container.sif
```


## Troubleshooting

- **Pickle error on compute node**: Ensure same python version on apptainer and login node. Ensure `hydra-apptainer-launcher` is installed inside the container.

## License

MIT
